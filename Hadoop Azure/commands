Hadoop Cluster  Commands - 42




sudo apt update
sudo apt install ssh
sudo apt install pdsh
nano .bashrc
export PDSH_RCMD_TYPE=ssh
ssh-keygen -t rsa -P ""
cat ~/.ssh/id_rsa.pub >> ~/.ssh/authorized_keys
ssh localhost
sudo apt install openjdk-8-jdk
java -version
sudo wget -P ~ https://mirrors.sonic.net/apache/hadoop/common/hadoop-3.2.1/hadoop-3.2.1.tar.gz
tar xzf hadoop-3.2.1.tar.gz
mv hadoop-3.2.1 hadoop
nano ~/hadoop/etc/hadoop/hadoop-env.sh

Goes in Hadoop-env.sh

	export JAVA_HOME=/usr/lib/jvm/java-8-openjdk-amd64/



sudo mv hadoop /usr/local/hadoop
sudo nano /etc/environment
	
	PATH="/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games:/usr/local/hadoop/bin:/usr/local/hadoop/sbin"JAVA_HOME="/usr/lib/jvm/java-8-openjdk-amd64/jre"


sudo adduser hadoopuser
sudo usermod -aG hadoopuser hadoopuser
sudo chown hadoopuser:root -R /usr/local/hadoop/
sudo chmod g+rwx -R /usr/local/hadoop/
sudo adduser hadoopuser sudo

sudo nano /etc/hosts
sudo nano /etc/hostname
sudo reboot



su - hadoopuser
ssh-keygen -t rsa



ssh-copy-id hadoopuser@hadoop-master
ssh-copy-id hadoopuser@hadoop-slave1
ssh-copy-id hadoopuser@hadoop-slave2


sudo nano /usr/local/hadoop/etc/hadoop/core-site.xml


<property>
<name>fs.defaultFS</name>
<value>hdfs://hadoop-master:9000</value>
</property>


sudo nano /usr/local/hadoop/etc/hadoop/hdfs-site.xml




<property>
<name>dfs.namenode.name.dir</name><value>/usr/local/hadoop/data/nameNode</value>
</property>
<property>
<name>dfs.datanode.data.dir</name><value>/usr/local/hadoop/data/dataNode</value>
</property>
<property>
<name>dfs.replication</name>
<value>2</value>
</property>




sudo nano /usr/local/hadoop/etc/hadoop/workers

hadoop-slave1
hadoop-slave2
hadoop-slave3
hadoop-slave4

scp /usr/local/hadoop/etc/hadoop/* hadoop-slave1:/usr/local/hadoop/etc/hadoop/
scp /usr/local/hadoop/etc/hadoop/* hadoop-slave2:/usr/local/hadoop/etc/hadoop/
scp /usr/local/hadoop/etc/hadoop/* hadoop-slave3:/usr/local/hadoop/etc/hadoop/
scp /usr/local/hadoop/etc/hadoop/* hadoop-slave4:/usr/local/hadoop/etc/hadoop/


source /etc/environment
hdfs namenode -format




start-dfs.sh







export HADOOP_HOME="/usr/local/hadoop"
export HADOOP_COMMON_HOME=$HADOOP_HOME
export HADOOP_CONF_DIR=$HADOOP_HOME/etc/hadoop
export HADOOP_HDFS_HOME=$HADOOP_HOME
export HADOOP_MAPRED_HOME=$HADOOP_HOME
export HADOOP_YARN_HOME=$HADOOP_HOME

sudo nano /usr/local/hadoop/etc/hadoop/yarn-site.xml

<property>
<name>yarn.resourcemanager.hostname</name>
<value>hadoop-master</value>
</property>


start-yarn.sh






